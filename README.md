# Math 120 Final Project: Dune Text Analysis

## Project Overview

This project involves analyzing Frank Herbert's _Dune_, using word frequency and sentiment analysis to observe trends for each chapter. It will find the most common words per chapter, the most common words in the novel, and the changes in sentiment as the chapters progress. There will also be visualizations and organized data sets.

## Dataset

The data for this project originally comes from this pdf: https://i.4pcdn.org/tg/1439541465764.pdf.

### References

Herbert, Frank. *Dune*. Ace Books, 2005.

## Folder Structure

# ***FIXME: ADD WRITEUP FILE***

```
python_final_project/
├── data_raw/               # Raw, unprocessed data files
│   └── Dune.txt              # Original text of Dune
├── data/                   # Cleaned and processed data
│   ├── sentiment_and_word_count.csv  # Combined data on sentiments and word counts (generated by notebook)
│   └── separated_and_tokenized.csv   # Separated chapters and cleaned/tokenized words (generated by notebook)
├── src/                    # Helper functions and modules
│   ├── __init__.py           # Package initialization
│   ├── dataProcessing.py     # Data loading and cleaning functions
│   └── analysis.py           # Data analysis and visualization functions
├── notebooks/              # Project notebooks
│   └── Final_Project.ipynb   # Final analysis notebook
└── README.md               # This file
```

## Running the Notebook

### Google Colab Execution
1. Open [Google Colab](https://colab.research.google.com/)
2. Upload `Final_Project.ipynb`
3. Run the first two cells to set up the environment

### Local Execution
1. Clone this repository:
   ```
   git clone
   ```
2. 
